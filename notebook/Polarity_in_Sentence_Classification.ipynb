{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "7i0hBQ1gvJQ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZbduMSNe-4Z",
        "outputId": "f8a02fa4-d52e-4590-8c41-1de4fd364ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Id                                           sentence  polarity_flag\n",
            "0  E05480-20  機器売上高は、前年同期比8.6％減の2,994百万円(前年同期 3,275百万円)、売上原価...              0\n",
            "1   E01703-1  このような状況の中、当社グループは顧客満足度の向上を経営方針として掲げ、新たな価値の創出にチ...              0\n",
            "2   E01946-7  平成28年度における連結業績は、売上高が5,369億42百万円（前期比13.9％減)、経常利...              0\n",
            "3  E00457-21  日清食品チルド㈱の販売状況は、主力ブランド「行列のできる店のラーメン」を中心としたラーメン群...              1\n",
            "4  E01743-15  設備工事等が増加したことにより、受注高は195億21百万円（前連結会計年度比15.8％増）と...              1\n",
            "          Id                                           sentence\n",
            "0  E00686-26                    このほか、セメント、製粉、砂糖・甘味、塩等の用途が減少しました\n",
            "1  E01054-16  その結果、当連結会計年度の売上高は92億59百万円（前年同期比１億81百万円、2.0%増）と...\n",
            "2   E04298-8  これに、特別損益を加減した税金等調整前当期純損失は128百万円（前年同期は276百万円の利益...\n",
            "3  E21200-20  油圧機器につきましては、建設機械向けシリンダーは堅調に推移したものの、掘削機用ジャッキと免制...\n",
            "4  E03991-10  マンション業界は、マイナス金利政策の導入や住宅取得税制の維持により、需要は堅調に推移しており...\n"
          ]
        }
      ],
      "source": [
        "# ファイルパス\n",
        "train_path = \"train.csv\"\n",
        "test_path = \"test.csv\"\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# データの確認（先頭5行を表示）\n",
        "print(train_df.head())\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wyik2ElyfkyR"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(columns=[\"Id\"])\n",
        "test_df = test_df.drop(columns=[\"Id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "0ZdCqR1Wfnfl"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.dropna(subset=[\"sentence\"])\n",
        "test_df = test_df.dropna(subset=[\"sentence\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CdbUOdijft10"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "train_df[\"sentence\"] = train_df[\"sentence\"].apply(clean_text)\n",
        "test_df[\"sentence\"] = test_df[\"sentence\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfOdZFggf3P1",
        "outputId": "89ad8d96-000f-423d-db2b-289ba9a60b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.11/dist-packages (1.0.10)\n",
            "Requirement already satisfied: unidic-lite in /usr/local/lib/python3.11/dist-packages (1.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install mecab-python3 unidic-lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "FJ0L1raAfyDA"
      },
      "outputs": [],
      "source": [
        "import MeCab\n",
        "\n",
        "mecab = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "def tokenize(text):\n",
        "    return mecab.parse(text).strip()\n",
        "\n",
        "train_df[\"sentence\"] = train_df[\"sentence\"].apply(tokenize)\n",
        "test_df[\"sentence\"] = test_df[\"sentence\"].apply(tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ9N6-P0gJr4",
        "outputId": "463bc8e8-41ac-4891-a1fd-e9beb7d8cc39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "これ は 日本 語 の 形態 素 解析 の テスト です 。 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import MeCab\n",
        "\n",
        "mecab = MeCab.Tagger(\"-Owakati\")\n",
        "text = \"これは日本語の形態素解析のテストです。\"\n",
        "print(mecab.parse(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "cJ-hiKoMgNXl"
      },
      "outputs": [],
      "source": [
        "#\n",
        "def tokenize(text):\n",
        "    return mecab.parse(text).strip()\n",
        "\n",
        "# データに適用\n",
        "train_df[\"sentence\"] = train_df[\"sentence\"].apply(tokenize)\n",
        "test_df[\"sentence\"] = test_df[\"sentence\"].apply(tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZC3sqXOmbWb",
        "outputId": "d7fdaf18-060b-4bde-d69e-f84e8342f456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zeCpi_KIqJ7i"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF の計算\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit(train_df[\"sentence\"])\n",
        "word_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
        "\n",
        "# TF-IDF の重み付き Word2Vec ベクトル\n",
        "def get_weighted_vector(sentence, model, weights):\n",
        "    vectors = [model.wv[word] * weights.get(word, 1) for word in sentence if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
        "\n",
        "# 新しいベクトル作成\n",
        "X_train_w2v_tfidf = np.vstack(train_tokens.apply(lambda x: get_weighted_vector(x, word2vec_model, word_weights)))\n",
        "X_test_w2v_tfidf = np.vstack(test_tokens.apply(lambda x: get_weighted_vector(x, word2vec_model, word_weights)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_wG_JI2qjWG",
        "outputId": "561bf227-c178-4f39-e013-f0cadc43ef78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 1324, number of negative: 733\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013453 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 25500\n",
            "[LightGBM] [Info] Number of data points in the train set: 2057, number of used features: 100\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.643656 -> initscore=0.591267\n",
            "[LightGBM] [Info] Start training from score 0.591267\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "TF-IDF + Word2Vec LightGBM AUC Score: 0.6712695389465388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "y_train = train_df[\"polarity_flag\"]\n",
        "\n",
        "# データ分割（学習データと検証データ）\n",
        "X_train_split_tfidf, X_val_split_tfidf, y_train_split_tfidf, y_val_split_tfidf = train_test_split(\n",
        "    X_train_w2v_tfidf, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# LightGBM モデルの作成（TF-IDF + Word2Vec ベクトルを使用）\n",
        "lgbm_model_tfidf = LGBMClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=10,\n",
        "    num_leaves=50,\n",
        "    min_child_samples=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# モデルの学習\n",
        "lgbm_model_tfidf.fit(X_train_split_tfidf, y_train_split_tfidf)\n",
        "\n",
        "# 検証データでの予測\n",
        "y_val_pred_tfidf = lgbm_model_tfidf.predict_proba(X_val_split_tfidf)[:, 1]\n",
        "\n",
        "# AUCスコアの計算\n",
        "auc_score_tfidf = roc_auc_score(y_val_split_tfidf, y_val_pred_tfidf)\n",
        "print(\"TF-IDF + Word2Vec LightGBM AUC Score:\", auc_score_tfidf)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
